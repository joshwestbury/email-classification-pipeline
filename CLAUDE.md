# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

We currently maintain a custom NetSuite module called "Collection Notes." This module is built on a custom record type that allows users to document and track collection activities, such as late or missed payments. It also stores email correspondence between NetSuite collection agents and customers, ensuring that all collection-related interactions are centralized and accessible.

### Project Goal

The goal of this project is to enhance the Collection Notes module with AI-powered classification and summarization. Specifically, we want to automatically analyze incoming customer emails in NetSuite, classify them by Intent and Sentiment, and generate actionable notes or recommendations for the collections team.

This automation will likely be implemented via a User Event Script that is triggered whenever an inbound email is received. The script will send the email content, along with a detailed system prompt, to a Large Language Model (LLM). The LLM will classify the email and return structured metadata (intent and sentiment), a concise summary, and suggested next steps for the agent.

### Classification Dimensions

**Intent**

The intent represents the underlying purpose of the customer's email — essentially, what they want to achieve. Common intent categories include:
• Request Action – asking the recipient to perform a specific task.
• Share Information – providing updates, reports, or FYI content.
• Seek Information – asking questions or requesting clarification.
• Build Relationship – expressing gratitude, maintaining rapport, or networking.
• Persuade/Influence – attempting to convince the recipient of a viewpoint or proposed action.
• Schedule/Coordinate – arranging meetings, calls, or aligning on timing.

**Sentiment**

The sentiment represents the emotional tone conveyed in the customer's message — in other words, how they feel. Sentiment is measured along multiple dimensions:
• Polarity – positive, negative, or neutral.
• Intensity – the strength of the expressed emotion (mild, moderate, strong).
• Specific Emotions – such as frustrated, urgent, apologetic, grateful, enthusiastic, or concerned.

### LLM System Prompt

The system prompt will contain:
• Definitions and descriptions of each intent and sentiment category.
• Examples of emails mapped to those categories.
• Instructions for how the LLM should classify emails consistently.

In addition to classification, the LLM will be instructed to:
• Generate a concise summary of the email.
• Recommend a course of action or next step for the collection agent.

### Prerequisite: Defining Taxonomy and System Prompt

Before integrating this functionality into NetSuite, we need to finalize the taxonomy of intent and sentiment categories and develop the detailed system prompt. To do this, we are building a classification pipeline that processes real-world emails and generates candidate categories.

### Python Best Practices

- **Style:** Follow PEP 8 rigorously. Use Black for consistent formatting.
- **Type Hinting:** Use PEP 484 type annotations for all functions and class methods.
- **Docstrings:** Use Google-style docstrings for all public functions and classes.
- **Error Handling:** Implement robust error handling with custom exceptions where appropriate.
- **Testing:** Write comprehensive unit tests for all modules using `pytest`.
- **Dependency Management:** Use `uv` for dependency management and virtual environments. Never use `pip`.
- **Git:** When writing commit messages never inlcude anything like "Generated by Claude" in the commit message.

### CRITICAL: Organic Taxonomy Discovery Rules

**NEVER HARDCODE CATEGORY NAMES OR DEFINITIONS IN THE PIPELINE CODE**

The entire purpose of this project is to discover taxonomies organically from email data. Any hardcoding of category names, definitions, examples, or consolidation rules defeats this purpose and creates bias.

**Strictly Prohibited:**
- ❌ Hardcoded category names (e.g., "Payment Inquiry", "Cooperative", "Frustrated")
- ❌ Predefined category definitions or descriptions
- ❌ Hardcoded examples mapped to specific category names
- ❌ LLM prompts that suggest or mandate specific category names
- ❌ Consolidation rules that prescribe specific category mergers by name
- ❌ Any code that assumes or expects specific category names to exist

**Required Approach:**
- ✅ LLM discovers categories purely from analyzing email clusters
- ✅ Category names emerge from the data patterns observed
- ✅ Consolidation uses semantic similarity, not prescribed names
- ✅ System prompts describe the PROCESS, not the expected OUTPUT
- ✅ All category-related code must work with ANY category names
- ✅ Generic consolidation logic (e.g., "merge similar categories") without naming them

**Verification:**
- Before committing code, search for any specific category names in the codebase
- Ensure LLM prompts only describe analysis methodology, not expected categories
- Test with different datasets to verify different taxonomies can emerge
- Categories in `taxonomy.yaml` should vary significantly across different email datasets

If you find yourself typing a specific category name in pipeline code, **STOP** - you are introducing bias.

## Project Phases

### Phase 1: Taxonomy Discovery ✅ **COMPLETED**

**Goal**: Develop a preliminary, reusable taxonomy of sentiment and intent categories from ~1k real collection emails.

**Status**: Complete - All deliverables generated and refined for production use

**Key Accomplishments**:
- Successfully clustered 703 incoming emails into 24 distinct groups
- Generated preliminary categories using GPT-4o analysis of top 8 clusters
- Refined taxonomy through human curation into production-ready categories
- Created comprehensive labeling guide with examples and decision rules

**Final Taxonomy**:
- **3 Intent Categories**: Payment Inquiry, Invoice Management, Information Request
- **4 Sentiment Categories**: Cooperative, Administrative, Informational, Frustrated
- **Business Value**: Clear actionable categories for NetSuite Collection Notes

**Deliverables**:

- `taxonomy_draft.json` — initial categories with examples ✅
- `taxonomy.yaml` — curated taxonomy with definitions and rules ✅
- `taxonomy_labeling_guide.md` — guide with examples and counterexamples ✅
- `cluster_analyses_llm.json` — detailed LLM analysis of email clusters ✅
- `master_email_threads_anonymized.json` — final processed dataset (5,693 emails) ✅

### Phase 2: Prototype Classifier **NEXT**

**Goal**: Build and validate an LLM-powered classifier that applies the taxonomy to real emails.

**Key Steps**:

1. Define strict JSON schema for structured outputs
2. Design few-shot prompts with system instructions
3. Manual labeling of 150-250 email subset for validation
4. Model performance testing (target ≥85% precision on top intents)
5. Iterate and refine based on confusion matrix analysis

**Deliverables**:

- `response_schema.json` — strict JSON schema for model output
- `emails_labeled.csv` — ground-truth labeled subset
- `confusion_matrix.png` — model vs human performance visualization
- `system_prompt.txt` — final prompt for production use

## Reusable Pipeline Architecture

The project now includes a modular pipeline system for processing multiple email datasets:

### Pipeline Components

- **`pipeline/`** — Modular components for reusable taxonomy discovery
- **`run_pipeline.py`** — CLI entry point for processing new datasets
- **`pipeline_config_template.yaml`** — Configuration template for new datasets
- **`PIPELINE_README.md`** — Usage documentation and examples

### Classification Pipeline Overview

The pipeline processes raw email data to derive meaningful taxonomies. Its components and workflow are as follows:

1. **Input**: Ingests a raw JSON file of emails.
2. **DataProcessor**: Normalizes fields, removes HTML formatting, trims quoted replies and signatures, and (optionally) adds limited thread context.
3. **Anonymizer**: Applies regex-based rules to detect and replace personally identifiable information (PII) such as emails, phone numbers, IDs, and names. A stable replacement map ensures consistency across documents.
4. **Embedder**: Uses sentence-transformers (default: all-MiniLM-L6-v2) to generate embeddings. Each message is represented as a vector derived from its subject line and body (length-limited).
5. **Clusterer**: Groups similar messages using UMAP for dimensionality reduction and HDBSCAN for density-based clustering. Produces cluster IDs while identifying noise/outliers.
6. **Analyzer**: Selects representative samples from each cluster and calls an LLM to:
   • Summarize the cluster's overall theme.
   • Propose preliminary intent and sentiment labels organically from email data.
   • Provide examples and rationales.
7. **Curator**: Performs two-stage taxonomy refinement:
   • **Stage 1**: Extract granular taxonomy from LLM analysis (typically 30-40 categories)
   • **Stage 2**: Apply aggressive LLM-based consolidation to reduce to 3-5 intent and 3-4 sentiment categories
   • Generate final taxonomy, labeling guide, and production system prompt
8. **Pipeline/Config**: Centralizes configuration (UMAP/HDBSCAN parameters, model settings, output paths). Exports results in JSON, CSV, and YAML for human review.

### Using the Pipeline

**Quick Start:**
```bash
# Create template configuration
python run_pipeline.py --create-template

# Process new email dataset
python run_pipeline.py --input new_emails.json --dataset-name customer_q4

# Use custom configuration
python run_pipeline.py --config my_dataset.yaml
```

Each dataset gets organized outputs in `outputs/{dataset_name}/` for easy management.

## Data Structure

### Final Production Taxonomy (Phase 1 Results)

**Intent Categories:**
- **Payment Inquiry** — Questions about payment status, methods, or schedules
- **Invoice Management** — Requests for invoice details, corrections, or documentation
- **Information Request** — General information requests not related to payments/invoices

**Sentiment Categories:**
- **Cooperative** — Willing to work together, positive engagement
- **Administrative** — Neutral, business-focused communication
- **Informational** — Seeking or providing factual information
- **Frustrated** — Expressing dissatisfaction or impatience

*See `taxonomy.yaml` and `taxonomy_labeling_guide.md` for detailed definitions and examples.*

### Legacy Categories (Pre-Phase 1)

*These were preliminary examples from initial analysis - superseded by the production taxonomy above:*

- apologetic, dispute, in process, acknowledgement, unknown

### Expected Output Schema

The classifier will output structured JSON containing:

- `intent` (one value from taxonomy)
- `tone` (one value from sentiment set)
- `modifiers` (urgency, commitment strength, risk flags)
- `extracted_entities` (dates, amounts, invoice IDs)
- `rationale` (short natural language explanation)

### Collection Note Record Fields

- **Note Type field**: Automated (can't be edited in UI) or User Generated
- **Sentiment**: Picklist based on the taxonomy categories

## Dataset Information

### Original Sample Data
- `litera_raw_emails.json`: Original sample email data (4,697 emails)

### Processed Datasets
- `master_email_threads.json`: Separated email threads (5,693 individual emails)
- `master_email_threads_anonymized.json`: Final anonymized dataset for analysis
- `incoming_email_embeddings.npy`: Vector embeddings for 703 incoming emails
- `thread_context_embeddings.npy`: Context embeddings for 372 email threads

### Pipeline Outputs
When using the pipeline with new datasets, outputs are organized in:
```
outputs/{dataset_name}/
├── processed_emails.json      # Cleaned and structured email data
├── anonymized_emails.json     # PII-removed dataset
├── embeddings/                # Vector representations
├── cluster_results.json       # Clustering analysis results
├── taxonomy_analysis.json     # Granular LLM-proposed categories (30-40 categories)
├── taxonomy.yaml              # Final consolidated taxonomy (3-5 intents, 3-4 sentiments)
├── taxonomy_labeling_guide.md # Human-readable classification guide
├── system_prompt.txt          # Production-ready NetSuite classification prompt
└── pipeline_summary.json     # Run summary and metrics
```

## Development Workflow

When working on this project:

1. **For new datasets**: Use the pipeline system (`run_pipeline.py`) for consistent processing
2. **Phase 2 development**: Build on the completed Phase 1 taxonomy and deliverables
3. Ensure all sensitive data is properly anonymized during processing
4. Maintain version control for taxonomy files and prompts
5. Document model performance metrics and validation results
6. Prepare deliverables for eventual NetSuite User Event integration

## Current Status

• The pipeline is **production-ready** with automated organic discovery and LLM consolidation.
• The system generates complete taxonomies from raw email data with minimal human intervention.
• Key features implemented:
  - Organic category discovery from email clusters (no predefined categories)
  - Aggressive LLM consolidation to business-focused taxonomies
  - Production system prompt generation for NetSuite integration
• Ready for Phase 2: Classifier validation and NetSuite integration.

### Pipeline Usage for New Datasets

```bash
# Quick start with new email dataset
python run_pipeline.py --input new_emails.json --dataset-name dataset_name

# Custom configuration approach
python run_pipeline.py --create-template
# Edit the generated template
python run_pipeline.py --config my_config.yaml
```

